% Methodology
% Be specific enough that someone could reproduce your work

\label{sec:methodology}

% Overview
Our goal was to examine how the same topics on Wikipedia are represented differently across multiple languages. To achieve this, we developed a Python-based program that uses the Wikipedia API to extract short summaries of topics in English, Japanese, and Spanish. The analysis focused on identifying variations in language use, tone, and emphasis to explore how linguistic and cultural factors may influence content presentation. This approach enabled us to directly compare multilingual outputs for the same subject under consistent retrieval conditions.

\subsection{Data Collection}

We collected data using the Wikipedia API, which provides structured access to article content through simple HTTP queries. For each test case, a topic name (such as “Politics,” “Climate Change,” or “History of Japan”) was entered under the \texttt{page} variable, and the program retrieved a 500-character summary. The same topic was then queried in different language editions—English, Japanese, and Spanish—by specifying the appropriate language code. This dataset provided side-by-side textual samples for qualitative comparison. All data were gathered between October and November 2025.

\subsection{Data Processing}

Because the Wikipedia API returns clean text snippets, minimal preprocessing was required. We stored the outputs in local text files for comparison. The main processing step involved aligning the English, Japanese, and Spanish summaries for each topic and reviewing them manually to note differences in phrasing, terminology, and emphasis. We also verified that each summary corresponded to the correct topic and language version. Future iterations of this project may include translation normalization or tokenization to support automated linguistic analysis.

\subsection{Analysis Methods}

Our analysis was primarily qualitative. We compared summaries across the three language editions to identify differences in word choice, tone, and focus. Each team member independently reviewed the results, highlighting variations in emphasis (e.g., historical framing, political orientation, or omission of details). These findings were then discussed collectively to determine recurring patterns of linguistic bias. While no computational models were used at this stage, this manual approach provided insight into the types of bias that could later be quantified using natural language processing (NLP) tools such as sentiment or keyword frequency analy
